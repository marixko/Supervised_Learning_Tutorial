{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Basics of Supervised Learning For Astronomers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marixko/Supervised_Learning_Tutorial/blob/master/The_Basics_of_Supervised_Learning_For_Astronomers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PHx36P_-TsGf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**About Google's Colaboratory: **\n",
        "\n",
        "This is a free Jupyter environment that runs in Google's cloud, which means you can run codes in your computer without having to install anything. You can create a copy of this tutorial in your own Google's Drive and make your own changes. Colaboratory also allows you to easily share your code with others! [Read more](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "C5t_fXnVn92u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "> **Author**: Lilianne M. I. Nakazono (email: lilianne.nakazono@usp.br) \n",
        "\n",
        "> PhD student at Instituto de Astronomia, Geofísica e Ciências Atmosféricas -- Universidade de São Paulo (IAG-USP). Bachelor's degree in Statistics (IME-USP) and in Astronomy (IAG-USP). \n",
        "\n",
        "> **April 2019**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8qPQLbuRrIIy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###**What is Machine Learning?**\n",
        "\n",
        "From SAS: \n",
        "\n",
        ">> *\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"*\n",
        "\n",
        "###**What is Supervised Learning?**#\n",
        "\n",
        "From S.B. Kotsiantis (2007): \n",
        "\n",
        ">> *\"Every instance in any dataset used by machine learning algorithms is represented using the same set of features. The features may be continuous, categorical or binary. If instances are given with known labels (the corresponding correct outputs) then the learning is called *supervised*, in contrast to *unsupervised learning*, where instances are unlabeled.\"*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**STAR/GALAXY separation**#\n",
        "\n",
        "In this tutorial we will perform a STAR/GALAXY separation using a real dataset from [S-PLUS](http://www.splus.iag.usp.br/). This data were already matched with [SDSS](https://www.sdss.org/) (DR15) spectroscopical data and it will be used to train and test the supervised classifiers. The final step (not included in this tutorial) is to use the trained model to predict the classification of your unknown objects.\n",
        " \n",
        " This tutorial will be entirely in Python 3 and we will go through the following topics:\n",
        "- Introduction to `Pandas` ([Documentation](https://pandas.pydata.org/))\n",
        "- Data visualization with `seaborn` ([Documentation](https://seaborn.pydata.org/))\n",
        "- Classification methods with `sklearn` ([Documentation](https://scikit-learn.org/stable/index.html))\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "03kTZpZNlsM2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Additional information about the data**\n",
        "\n",
        "\n",
        "\n",
        "ID        -            Object ID Number\n",
        "\n",
        "RA         -           Right Ascension in decimal degrees [J2000]\n",
        "\n",
        "Dec          -         Declination in decimal degrees  [J2000]\n",
        "\n",
        "FWHM_n       -         Normalized Full width at half maximum to detection image seeing (pixels)\n",
        "\n",
        " A        -             Profile RMS along major axis (pixels)\n",
        " \n",
        "B         -            Profile RMS along minor axis (pixels)\n",
        "\n",
        "KrRadDet      -        Kron apertures in units of A or B (pixels)\n",
        "\n",
        "uJAVA_auto, F378_auto, F395_auto, F410_auto, F430_auto, g_auto, F515_auto, r_auto, F660_auto, i_auto, F861_auto, z_auto      -          Total-restricted magnitudes  (AB) in corresponding filters\n",
        "\n",
        "class - Spectroscopic classification from SDSS\n"
      ]
    },
    {
      "metadata": {
        "id": "Uy3zRaFzeTcu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**1. Libraries and Functions**\n"
      ]
    },
    {
      "metadata": {
        "id": "oF_R5LwQvTtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib as mpl \n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn import metrics\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diRUtxhyr0-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modified from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.3f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iOGSCuOYvUg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**2. Read Data**\n",
        "\n",
        "For statistical and machine learning purposes it is **always**  better to read the data in a dataframe (data structured in labels for rows and columns) format.\n"
      ]
    },
    {
      "metadata": {
        "id": "YAIruqFSkz9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reading dataset from github and saving as dataframe\n",
        "url = 'https://raw.githubusercontent.com/marixko/'\n",
        "file = 'tutorial_classifiers/master/tutorial_data.txt'\n",
        "df = pd.read_csv(url+file, delim_whitespace=True, low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IKM7hf_nVK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this cell to quickly check your dataset\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qIaZmNhsT29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check header\n",
        "list(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mD-bx0hLpmrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**3. Pre-analysis**"
      ]
    },
    {
      "metadata": {
        "id": "dDz8Gvvn10fy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Before applying any kind of analysis, you need to be aware of any problem in your dataset that can affect your training (e.g. missing values and outliers). Sometimes it will require pre-processing your dataset beforehand (e.g. for missing values, interpolating values or removing them from data may be necessary). "
      ]
    },
    {
      "metadata": {
        "id": "kSIq3AwVh6dZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You can check your dataset by using describe(). \n",
        "# It will return the total count, mean, standard deviation,\n",
        "# minimum, Q1, Q2 (median), Q3 and maximum\n",
        "\n",
        "df.describe()\n",
        "\n",
        "# If you want to check a specific feature use for instance:\n",
        "\n",
        "# df.FWHM_n.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7kEvO87GOGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another good practice is to check high correlations in your dataset, which can allow you to identify which features are redundant. Thus, you can also be able to reduce dimensionality of your dataset.\n",
        "\n",
        "\n",
        ">> *\"The fact that many features depend on one another often unduly influences the accuracy of supervised ML classification models. This problem can be addressed by construction new features from the basic feature set.\"* -- S.B. Kotsiantis (2007)\n",
        "\n",
        "(One way to deal with multicollinearity -- when 2 or more features are moderately or highly correlated -- is creating a new feature set using  [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).)"
      ]
    },
    {
      "metadata": {
        "id": "Vu2LDRH99P--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.close()\n",
        "f, ax = plt.subplots(figsize=(8, 8))\n",
        "var = ['FWHM_n', 'A', 'B', 'KrRadDet', 'uJAVA_auto', \n",
        "       'F378_auto', 'F395_auto', 'F410_auto', 'g_auto', 'F515_auto',\n",
        "       'r_auto', 'F660_auto', 'i_auto', 'F861_auto', 'z_auto']\n",
        "corr = df[var].corr()\n",
        "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), \n",
        "            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            square=True, ax=ax, center=0, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "#It would also be interesting to check the correlation plot for each class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KTQJKAe4ZkG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Qualitative variables can also be included. In this case, however, there are no qualitative features that came from S-PLUS observations.\n",
        "But let's check the classification label counts:"
      ]
    },
    {
      "metadata": {
        "id": "InBPLjPoUDOW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For qualitative variables, use value_counts()\n",
        "df['class'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhTzOGPowOUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that for this example the classes are balanced. It represents a best case scenario, which rarely happens in the real world. \n",
        "\n",
        "Be very careful with imbalanced datasets! Some methods and metrics are not good for imbalanced cases, some manipulation in your sampling method (e.g. over/under-sampling) or in your algorithm (e.g. penalized classification)  may be necessary. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J-jVys-Maevc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Note:** Supervised Learning is not suitable for problems like \"I want to find very rare objects that we have never found before!\". The learning process is based on your ground-truth samples, so you need to ask yourself \"Is my ground-truth sample representative of what I want to find?\""
      ]
    },
    {
      "metadata": {
        "id": "CqRJix-iiNuL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#** 4. Feature Selection**"
      ]
    },
    {
      "metadata": {
        "id": "r6-pv4w0lcvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A very important step of the analysis is choosing your input features. Sometimes you already know which features you need to use to achieve your goals, which comes from your previous knowledge about the topic. However, you can also evaluate which features will give you the best performance. We will discuss more about it on the following sections. \n",
        "\n",
        "For didactic purposes, let's consider two feature spaces:\n",
        "\n",
        "> `dim15` = {all useful information from the catalog}\n",
        "\n",
        "> `dim2` = {normalized FWHM, Profile RMS along major axis}"
      ]
    },
    {
      "metadata": {
        "id": "RgZSJv-EldBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dim15 = ['FWHM_n', 'A', 'B', 'KrRadDet', 'uJAVA_auto', \n",
        "       'F378_auto', 'F395_auto', 'F410_auto', 'g_auto', 'F515_auto',\n",
        "       'r_auto', 'F660_auto', 'i_auto', 'F861_auto', 'z_auto']\n",
        "\n",
        "dim2 = ['FWHM_n','A']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkN82SwFv-7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#** 5. Sampling training and testing sets **"
      ]
    },
    {
      "metadata": {
        "id": "0ysZB1qnJYK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Regardless of the classification method you choose, you will want to estimate how accurately your predictive model will perform. This is called **cross-validation** and there are several ways to do it. Some examples are:\n",
        "\n",
        "* **Holdout method**: randomly separate your original dataset into the training and the testing set. It's very common to adopt 1:3 ratio for the size of test/training sets, although you can choose another ratio. Very simple and fast computationally, but you need to be cautious as it is a single run method. Thus, it may be subject to large variabilities\n",
        "\n",
        "* **Leave-p-out cross-validation**:\n",
        "Uses p observations as the testing set and the remaining observations as the training set. Repeat to cover any sampling possibility\n",
        "\n",
        "* **k-fold cross-validation**: the original dataset is randomly partitioned into k equal sized subsamples. One subsample will be used as testing set and the other k-1 as training set. Repeat k times, until each subsample is used exactly once as the testing set.\n",
        "\n",
        "\n",
        "I strongly recommend that you also check the other methods before choosing one. For this tutorial we will use the **Holdout method**, for simplicity."
      ]
    },
    {
      "metadata": {
        "id": "gt2qSm1pwAjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label = pd.DataFrame(df['class'])\n",
        "\n",
        "# Transform strings into numbered labels\n",
        "label.loc[label['class'] == 'STAR', 'class'] = 0\n",
        "label.loc[label['class'] == 'GALAXY', 'class'] = 1\n",
        "\n",
        "# Use train_test_split() to sample your training and testing sets\n",
        "# Let's fix a random_state=42 in order to have the same sets \n",
        "# on each run. Stratify parameter guarantees that the original \n",
        "# proportion of the classes is maintained \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[dim15], label, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=42,\n",
        "                                                   stratify = label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu1OgvP7eHIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#** 6. Classification method: Support Vector Machine (SVM)**"
      ]
    },
    {
      "metadata": {
        "id": "nXY0Qo_UeQO9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We finally reached the point where we are going to run a classification algorithm. It is common to think, at first, that this would be the most complicated part, but a well-done job will require you to spend most of your time on the other steps. \n",
        "\n",
        "There are several classification methods you can use, each of them has its own pros and cons, depending on your science goals and on your dataset. I will give you an example using Support Vector Machine (SVM) with linear kernel, but I recommend you to also check other methods (e.g. Random Forest, Logistic Regression, K-NN, ...)\n",
        "\n",
        "**DON'T FORGET TO:**\n",
        " \n",
        " - Learn the basic idea of the method. You don't need to know all the math behind it, but you need to know how it works intuitively\n",
        " - Check what are the assumptions of the method and if your dataset is in agreement with it\n",
        " - Learn what the parameters of your model (a.k.a. hyperparameters) do. Choosing them wisely can be crucial to have good results in the end. Note: the hyperparameters space can also be part of your validation tests"
      ]
    },
    {
      "metadata": {
        "id": "Wfyau40zSlVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1. Basic idea"
      ]
    },
    {
      "metadata": {
        "id": "149IEmaDU8vO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The SVM finds the hyperplane that best separates your data, based on maximizing the margin between each class. For instance, in one dimension SVM will find a point. For two dimensions, it will be a line. For three dimensions, it will be a plane.\n",
        "\n",
        "To use a linear kernel, we assume that the data is linearly separable. Otherwise, we should use another kernel (e.g. polynomial).\n",
        "\n",
        "\n",
        "Read more about SVM [here](https://scikit-learn.org/stable/modules/svm.html#scores-probabilities)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sCMu4S2pnehR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2. Feature space: dim2"
      ]
    },
    {
      "metadata": {
        "id": "esHWuSDCeNQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train your model:\n",
        "clf2 = SVC(kernel= 'linear')\n",
        "clf2.fit(X_train[dim2], y_train.values.ravel()) \n",
        "\n",
        "# Make the predictions: \n",
        "y_pred2 = clf2.predict(X_test[dim2])\n",
        "\n",
        "# Plot confusion matrix:\n",
        "matrix = confusion_matrix(y_test['class'], y_pred)\n",
        "fig = plot_confusion_matrix(matrix, classes=['STAR','GALAXY'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlGoloaMGT25",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the confusion matrix above we can already see how good the results are: most of our stars (galaxies) are being assigned as stars (galaxies) and just a few percent were misclassified.\n",
        "\n",
        "Now let's check the plot and how the separation looks like:"
      ]
    },
    {
      "metadata": {
        "id": "TGR7PJYR82a8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-pastel')\n",
        "fig = plt.figure(figsize=(18,6))\n",
        "gs = gridspec.GridSpec(1, 2)\n",
        "ax = plt.subplot(gs[0,0])\n",
        "sns.scatterplot(x=X_train.FWHM_n, y=X_train.A, \n",
        "                    hue=y_train['class'])\n",
        "\n",
        "#Calculate margin (from https://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html)\n",
        "w = clf2.coef_[0]\n",
        "a = -w[0] / w[1]\n",
        "xx = np.linspace(-5, 5)\n",
        "yy = a * xx - (clf2.intercept_[0]) / w[1]\n",
        "margin = 1 / np.sqrt(np.sum(clf2.coef_ ** 2))\n",
        "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
        "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
        "\n",
        "#Plot margin\n",
        "plt.plot(xx, yy, 'k-')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.xlabel('FWHM_n')\n",
        "plt.ylabel('A')\n",
        "plt.xlim(0,8)\n",
        "plt.ylim(0.8, 10)\n",
        "\n",
        "plt.title('Training set')\n",
        "\n",
        "\n",
        "ax = plt.subplot(gs[0,1])\n",
        "sns.scatterplot(x=X_test.FWHM_n , y=X_test.A, hue=y_test['class'])\n",
        "plt.plot(xx, yy, 'k-')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.xlim(0,8)\n",
        "plt.ylim(0.8, 10)\n",
        "plt.title('Testing set')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6F0PQFq7Goto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The solid line corresponds to the optimal threshold found by SVM. The dashed lines in the plots above correspond to the maximized margin that I mentioned in Section 6.1. \n",
        "\n",
        "These are calculated using only a small part of the data: the objects around where the separation may occur, they are called the Support Vectors. Let's check which ones were considered for this classification:"
      ]
    },
    {
      "metadata": {
        "id": "yMX3lOWc9p_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(9,7))\n",
        "sns.scatterplot(x=X_train[dim2].FWHM_n, y=X_train[dim2].A, \n",
        "                    hue=y_train['class'])\n",
        "\n",
        "plt.scatter(clf2.support_vectors_[:, 0], \n",
        "clf2.support_vectors_[:, 1], s=8, \n",
        "zorder=10,color='red', marker='+')\n",
        "\n",
        "plt.xlim(0.9,2)\n",
        "plt.ylim(0.8,5)\n",
        "plt.plot(xx, yy, 'k-')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.title('Support vectors (Training set)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LlrBxabnRqwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.3. Feature space: dim15"
      ]
    },
    {
      "metadata": {
        "id": "T7VFpcr9Rt-h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the last section we saw how SVM works in a 2D space. In that case, it is possible to visually check the separation. However, we have much more information available. if we analyse them altogether, it can improve our results. Although, it is impossible to visually check the results, so we need to rely on performance metrics that we will discuss further on the next section. \n"
      ]
    },
    {
      "metadata": {
        "id": "hMThJZoSRqKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train your model:\n",
        "clf15 = SVC(kernel= 'linear')\n",
        "clf15.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Make predictions:\n",
        "y_pred = clf15.predict(X_test)\n",
        "\n",
        "# Plot confusion matrix:\n",
        "matrix = confusion_matrix(y_test['class'], y_pred)\n",
        "fig = plot_confusion_matrix(matrix, classes=['STAR','GALAXY'])\n",
        "plt.show()\n",
        "\n",
        "# Yeah, as simple as that! :) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RS5-O0VGf8mS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#** 7. Validation and Model Selection**"
      ]
    },
    {
      "metadata": {
        "id": "nZ7QpQhfST97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How can we choose between two (or more) different models? \n",
        "\n",
        "For that, we have several performance metrics that we can consider when selecting the best model and I will show a few of them.\n",
        "\n",
        "The way you are going to analyze the metrics depends on your science goals. For instance: \n",
        "\n",
        "* In a STAR/GALAXY separation you are probably not interested in a specific class, but in the overall classification. You can evaluate your model using, for example, Accuracy or F-measure\n",
        "\n",
        "* Suppose you had a STAR/QSO problem instead, where your main goal is to find new QSOs. You can evaluate your model using, for example, Precision, Recall or F-measure. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8CTNAqOCVSvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.1 Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "vYMExkhssayR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defined as the fraction of correct predictions.\n",
        "\n",
        "(Note: accuracy will be biased towards the class with higher frequency, don't rely on this measurement if you have an imbalanced dataset)"
      ]
    },
    {
      "metadata": {
        "id": "fCuuwkOIVjYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy\")\n",
        "print(\" First model (dim2):\", \n",
        "      np.round(100*metrics.accuracy_score(y_test, y_pred2),2), '%')\n",
        "print(\" Second model (dim15):\", \n",
        "      np.round(100*metrics.accuracy_score(y_test, y_pred),2), '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifCvku6gVV2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.2. Precision"
      ]
    },
    {
      "metadata": {
        "id": "kILVe5504b1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defined as:\n",
        "\n",
        "> Precision $\\equiv \\frac{TP}{(TP+FP)}$\n",
        "\n",
        "TP - True Positive ; FP - False Positive\n",
        "\n",
        "Note that you need to define which class will be your \"positive\". For example:\n",
        "\n",
        " \n",
        "| STAR (predicted) | GALAXY (predicted)\n",
        "--- | ---\n",
        "**STAR** (true label) | True Negative | False Positive\n",
        "**GALAXY** (true label)| False Negative | True Positive\n",
        "\n",
        "\n",
        "In Astronomy, it's called **purity**."
      ]
    },
    {
      "metadata": {
        "id": "uUqQ2dFmWHNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "P2 = metrics.precision_score(y_test, y_pred2, pos_label=1)\n",
        "P = metrics.precision_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "print(\"Galaxy Precision\")\n",
        "print(\" First model (dim2):\", np.round(100*P2,2), '%')\n",
        "print(\" Second model (dim15):\", np.round(100*P,2), '%')\n",
        "\n",
        "# Exercise: Calculate star precision for each model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHtGACLHVYUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.3. Recall"
      ]
    },
    {
      "metadata": {
        "id": "OuC0WNPx4rKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defined as:\n",
        "\n",
        "> Recall $\\equiv \\frac{TP}{(TP+FN)}$\n",
        "\n",
        "TP - True Positive ; FN - False Negative\n",
        "\n",
        "In Astronomy, it's called **completeness**."
      ]
    },
    {
      "metadata": {
        "id": "K_nCp-98WOpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "R2 = metrics.recall_score(y_test, y_pred2, pos_label=1)\n",
        "R = metrics.recall_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "\n",
        "print(\"Galaxy Recall\")\n",
        "print(\" First model (dim2):\", np.round(100*R2,2), '%')\n",
        "print(\" Second model (dim15):\", np.round(100*R,2), '%')\n",
        "\n",
        "# Exercise: Calculate star recall for each model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHJnEEAYVaJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7.4. F-measure"
      ]
    },
    {
      "metadata": {
        "id": "XAonJIBq4rvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's the harmonic mean of Precision and Recall:\n",
        "\n",
        "$F = \\frac{1}{2}\\Big(P_i^{-1}+R_i^{-1}\\Big)^{-1}  = 2 \\times \\frac{P_iR_i}{P_i+R_i}, F  \\in [0,1]$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KA9e_wfuWUd1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"F-measure\")\n",
        "print(\" First model  (dim2):\", np.round(metrics.f1_score(y_test, y_pred2),3))\n",
        "print(\" Second model  (dim15):\", np.round(metrics.f1_score(y_test, y_pred),3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c7GRvEg9UfyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Final message"
      ]
    },
    {
      "metadata": {
        "id": "ya0Bzb65Lphq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We came to the end of this tutorial, yay! :)\n",
        "\n",
        "Although it is called \"Machine Learning\", you are still the one who is going to make crucial decisions. And that is hard work! I hope I was able to give you at least a brief idea of all the steps involved in the process. \n",
        "\n",
        "Now, play around with the code:\n",
        "* Try other algorithms with the same feature selection and compare your results using the performance metrics\n",
        "* Test changing the parameters of your model\n",
        "* Try it with your own dataset!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QeHxH0zAjQwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read more:\n",
        "\n",
        "[Supervised Machine Learning: A Review of Classification Techniques](https://books.google.com/books?hl=en&lr=&id=vLiTXDHr_sYC&oi=fnd&pg=PA3&dq=review+supervised+learning&ots=CYpwxt2Bnn&sig=Y79PK3w3Q8CefKaTh03keRFEwyg#v=onepage&q=review%20supervised%20learning&f=false) (S.B. Kotsiantis, 2007)\n",
        "\n",
        "An Empirical Comparison of Supervised Learning Algorithms Rich (Rich Caruana and Alexandru Niculescu-Mizil, 2006)\n",
        "\n",
        "Classification of Imbalanced Data: a Review (Yanmin Sun, Andrew K. C. Wong and Mohamed S. Kamel, 2009)\n",
        "\n",
        "\n",
        "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
        "\n",
        " [A Practical Guide to Support Vector Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) (Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin, 2016)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}